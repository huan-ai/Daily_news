"""
ç½‘é¡µæ•°æ®æ”¶é›†å™¨

çˆ¬å–å®˜æ–¹åšå®¢ç­‰ç½‘é¡µå†…å®¹
"""

import asyncio
import random
from datetime import datetime
from typing import List, Dict, Any, Optional
import httpx
from bs4 import BeautifulSoup

from .base import BaseCollector, NewsItem, NewsCategory
from ..utils.logger import log


class WebCollector(BaseCollector):
    """
    ç½‘é¡µæ•°æ®æ”¶é›†å™¨
    
    çˆ¬å–å®˜æ–¹åšå®¢ã€æ–°é—»é¡µé¢ç­‰
    """
    
    # User-Agentåˆ—è¡¨
    USER_AGENTS = [
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15"
    ]
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__("Web", config)
        self.sources = self.config.get("sources", {})
        self.timeout = self.config.get("timeout", 30)
        self.interval = self.config.get("request_interval", 5)
        
    def _get_headers(self) -> Dict[str, str]:
        """è·å–éšæœºè¯·æ±‚å¤´"""
        return {
            "User-Agent": random.choice(self.USER_AGENTS),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br"
        }
        
    async def collect(self) -> List[NewsItem]:
        """
        æ”¶é›†ç½‘é¡µæ•°æ®
        
        Returns:
            æ”¶é›†åˆ°çš„æ–°é—»åˆ—è¡¨
        """
        items = []
        
        for source_name, source_config in self.sources.items():
            if not source_config.get("enabled", True):
                continue
            
            source_items = await self._collect_source(source_name, source_config)
            items.extend(source_items)
            
            await asyncio.sleep(self.interval)  # è¯·æ±‚é—´éš”
        
        log.info(f"ç½‘é¡µæ”¶é›†å®Œæˆï¼Œå…±{len(items)}æ¡")
        return items
    
    async def _collect_source(self, name: str, config: Dict[str, Any]) -> List[NewsItem]:
        """
        æ”¶é›†å•ä¸ªæ¥æº
        
        Args:
            name: æ¥æºåç§°
            config: æ¥æºé…ç½®
            
        Returns:
            æ–°é—»åˆ—è¡¨
        """
        items = []
        
        # æ”¶é›†åšå®¢
        if config.get("blog"):
            blog_items = await self._collect_blog(name, config["blog"])
            items.extend(blog_items)
        
        # å¯æ‰©å±•ï¼šæ”¶é›†å…¶ä»–ç±»å‹çš„é¡µé¢
        
        return items
    
    async def _collect_blog(self, source_name: str, blog_url: str) -> List[NewsItem]:
        """
        æ”¶é›†åšå®¢é¡µé¢
        
        Args:
            source_name: æ¥æºåç§°
            blog_url: åšå®¢URL
            
        Returns:
            æ–°é—»åˆ—è¡¨
        """
        items = []
        
        try:
            async with httpx.AsyncClient(timeout=self.timeout, follow_redirects=True) as client:
                response = await client.get(blog_url, headers=self._get_headers())
                
                if response.status_code != 200:
                    log.warning(f"åšå®¢ {source_name} è¯·æ±‚å¤±è´¥: {response.status_code}")
                    return items
                
                # æ ¹æ®æ¥æºé€‰æ‹©è§£ææ–¹æ³•
                parser = self._get_parser(source_name)
                if parser:
                    items = parser(response.text, source_name, blog_url)
                else:
                    items = self._generic_parse(response.text, source_name, blog_url)
                    
        except Exception as e:
            log.error(f"åšå®¢ {source_name} æ”¶é›†å¤±è´¥: {e}")
        
        return items
    
    def _get_parser(self, source_name: str):
        """
        è·å–ç‰¹å®šæ¥æºçš„è§£æå™¨
        
        Args:
            source_name: æ¥æºåç§°
            
        Returns:
            è§£æå‡½æ•°æˆ–None
        """
        parsers = {
            "anthropic": self._parse_anthropic,
            "google": self._parse_google,
            "meta": self._parse_meta,
            "deepmind": self._parse_deepmind,
        }
        return parsers.get(source_name.lower())
    
    def _parse_anthropic(self, html: str, source_name: str, base_url: str) -> List[NewsItem]:
        """è§£æAnthropicåšå®¢"""
        items = []
        soup = BeautifulSoup(html, "lxml")
        
        # Anthropicåšå®¢æ–‡ç« åˆ—è¡¨
        articles = soup.select("article, .post-card, [class*='blog-post']")
        
        for article in articles[:10]:
            try:
                # æ ‡é¢˜å’Œé“¾æ¥
                title_elem = article.select_one("h2, h3, .title, [class*='title']")
                link_elem = article.select_one("a[href]")
                
                if not title_elem:
                    continue
                
                title = title_elem.get_text(strip=True)
                link = link_elem.get("href", "") if link_elem else ""
                
                # è¡¥å…¨ç›¸å¯¹é“¾æ¥
                if link and not link.startswith("http"):
                    from urllib.parse import urljoin
                    link = urljoin(base_url, link)
                
                # æ‘˜è¦
                summary_elem = article.select_one("p, .excerpt, .summary, [class*='description']")
                summary = summary_elem.get_text(strip=True) if summary_elem else ""
                
                # æ—¥æœŸ
                date_elem = article.select_one("time, .date, [class*='date']")
                published_at = None
                if date_elem:
                    date_text = date_elem.get("datetime") or date_elem.get_text(strip=True)
                    published_at = self._parse_date(date_text)
                
                item = NewsItem(
                    title=f"ğŸ¤– {title}",
                    content=summary,
                    url=link or base_url,
                    source=f"Anthropic Blog",
                    published_at=published_at,
                    category=NewsCategory.MODEL_PROGRESS,
                    tags=["anthropic", "claude"],
                )
                
                if self.validate(item):
                    items.append(item)
                    
            except Exception as e:
                log.debug(f"è§£ææ–‡ç« å¤±è´¥: {e}")
                continue
        
        return items
    
    def _parse_google(self, html: str, source_name: str, base_url: str) -> List[NewsItem]:
        """è§£æGoogle AIåšå®¢"""
        items = []
        soup = BeautifulSoup(html, "lxml")
        
        articles = soup.select("article, .uni-blog-article, [class*='article']")
        
        for article in articles[:10]:
            try:
                title_elem = article.select_one("h2, h3, .title")
                link_elem = article.select_one("a[href]")
                
                if not title_elem:
                    continue
                
                title = title_elem.get_text(strip=True)
                link = link_elem.get("href", "") if link_elem else base_url
                
                if link and not link.startswith("http"):
                    from urllib.parse import urljoin
                    link = urljoin(base_url, link)
                
                summary_elem = article.select_one("p, .snippet")
                summary = summary_elem.get_text(strip=True) if summary_elem else ""
                
                item = NewsItem(
                    title=f"ğŸ” {title}",
                    content=summary,
                    url=link,
                    source="Google AI Blog",
                    category=NewsCategory.MODEL_PROGRESS,
                    tags=["google", "gemini"],
                )
                
                if self.validate(item):
                    items.append(item)
                    
            except Exception as e:
                log.debug(f"è§£ææ–‡ç« å¤±è´¥: {e}")
                continue
        
        return items
    
    def _parse_meta(self, html: str, source_name: str, base_url: str) -> List[NewsItem]:
        """è§£æMeta AIåšå®¢"""
        return self._generic_parse(html, "Meta AI", base_url)
    
    def _parse_deepmind(self, html: str, source_name: str, base_url: str) -> List[NewsItem]:
        """è§£æDeepMindåšå®¢"""
        return self._generic_parse(html, "DeepMind", base_url)
    
    def _generic_parse(self, html: str, source_name: str, base_url: str) -> List[NewsItem]:
        """
        é€šç”¨è§£æå™¨
        
        å°è¯•ä»ä»»æ„é¡µé¢æå–æ–‡ç« åˆ—è¡¨
        """
        items = []
        soup = BeautifulSoup(html, "lxml")
        
        # å¸¸è§çš„æ–‡ç« å®¹å™¨é€‰æ‹©å™¨
        selectors = [
            "article",
            "[class*='post']",
            "[class*='article']",
            "[class*='blog']",
            "[class*='card']",
            ".news-item",
            ".list-item"
        ]
        
        for selector in selectors:
            articles = soup.select(selector)
            if len(articles) >= 3:  # æ‰¾åˆ°è¶³å¤Ÿå¤šçš„æ–‡ç« 
                break
        else:
            articles = []
        
        for article in articles[:10]:
            try:
                # æŸ¥æ‰¾æ ‡é¢˜
                title_elem = (
                    article.select_one("h1, h2, h3, h4") or
                    article.select_one("[class*='title']") or
                    article.select_one("a")
                )
                
                if not title_elem:
                    continue
                
                title = title_elem.get_text(strip=True)
                if len(title) < 5:
                    continue
                
                # æŸ¥æ‰¾é“¾æ¥
                link_elem = article.select_one("a[href]") or title_elem.find_parent("a")
                link = link_elem.get("href", "") if link_elem else ""
                
                if link and not link.startswith("http"):
                    from urllib.parse import urljoin
                    link = urljoin(base_url, link)
                
                # æŸ¥æ‰¾æ‘˜è¦
                summary_elem = article.select_one("p")
                summary = summary_elem.get_text(strip=True) if summary_elem else ""
                
                item = NewsItem(
                    title=title,
                    content=summary or title,
                    url=link or base_url,
                    source=source_name,
                    category=NewsCategory.OTHER,
                    tags=[source_name.lower().replace(" ", "-")],
                )
                
                if self.validate(item):
                    items.append(item)
                    
            except Exception as e:
                continue
        
        return items
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """
        è§£ææ—¥æœŸå­—ç¬¦ä¸²
        
        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸²
            
        Returns:
            datetimeå¯¹è±¡æˆ–None
        """
        if not date_str:
            return None
        
        # å¸¸è§æ—¥æœŸæ ¼å¼
        formats = [
            "%Y-%m-%d",
            "%Y-%m-%dT%H:%M:%S",
            "%Y-%m-%dT%H:%M:%SZ",
            "%B %d, %Y",
            "%b %d, %Y",
            "%d %B %Y",
            "%d %b %Y",
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(date_str.strip(), fmt)
            except:
                continue
        
        return None


# æµ‹è¯•å…¥å£
if __name__ == "__main__":
    async def test():
        config = {
            "sources": {
                "anthropic": {
                    "blog": "https://www.anthropic.com/news",
                    "enabled": True
                }
            },
            "timeout": 30,
            "request_interval": 3
        }
        
        collector = WebCollector(config)
        items = await collector.collect()
        
        print(f"æ”¶é›†åˆ° {len(items)} æ¡æ–°é—»")
        for item in items[:5]:
            print(f"- {item.title}")
            print(f"  URL: {item.url}")
            print()
    
    asyncio.run(test())
