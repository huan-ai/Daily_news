"""
GitHubæ•°æ®æ”¶é›†å™¨

æ”¶é›†GitHubä¸Šçš„çƒ­é—¨AIé¡¹ç›®å’Œæœ€æ–°åŠ¨æ€
"""

import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional
import httpx
from bs4 import BeautifulSoup

from .base import BaseCollector, NewsItem, NewsCategory
from ..utils.logger import log


class GitHubCollector(BaseCollector):
    """
    GitHubæ•°æ®æ”¶é›†å™¨
    
    æ”¶é›†GitHub Trendingå’ŒæŒ‡å®šä»“åº“çš„æœ€æ–°åŠ¨æ€
    """
    
    TRENDING_URL = "https://github.com/trending"
    API_BASE = "https://api.github.com"
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__("GitHub", config)
        self.topics = self.config.get("topics", ["artificial-intelligence", "llm"])
        self.repos = self.config.get("repositories", [])
        self.since = self.config.get("since", "daily")
        
    async def collect(self) -> List[NewsItem]:
        """
        æ”¶é›†GitHubæ•°æ®
        
        Returns:
            æ”¶é›†åˆ°çš„æ–°é—»åˆ—è¡¨
        """
        items = []
        
        # æ”¶é›†Trendingé¡¹ç›®
        trending_items = await self._collect_trending()
        items.extend(trending_items)
        
        # æ”¶é›†æŒ‡å®šä»“åº“çš„æœ€æ–°release
        if self.repos:
            repo_items = await self._collect_repos()
            items.extend(repo_items)
        
        log.info(f"GitHubæ”¶é›†å®Œæˆï¼Œå…±{len(items)}æ¡")
        return items
    
    async def _collect_trending(self) -> List[NewsItem]:
        """
        æ”¶é›†GitHub Trending
        
        Returns:
            Trendingé¡¹ç›®åˆ—è¡¨
        """
        items = []
        
        try:
            async with httpx.AsyncClient(timeout=30) as client:
                # æ”¶é›†å„ä¸ªä¸»é¢˜çš„trending
                for topic in self.topics[:3]:  # é™åˆ¶ä¸»é¢˜æ•°é‡
                    url = f"{self.TRENDING_URL}?since={self.since}&spoken_language_code=&topic={topic}"
                    
                    response = await client.get(url, headers={
                        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)"
                    })
                    
                    if response.status_code == 200:
                        topic_items = await self._parse_trending_page(client, response.text, topic)
                        items.extend(topic_items[:5])  # æ¯ä¸ªä¸»é¢˜å–å‰5ä¸ª
                    
                    await asyncio.sleep(2)  # è¯·æ±‚é—´éš”
                
                # æ”¶é›†é€šç”¨trendingï¼ˆæ‰€æœ‰è¯­è¨€ï¼‰
                response = await client.get(
                    f"{self.TRENDING_URL}?since={self.since}",
                    headers={"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)"}
                )
                
                if response.status_code == 200:
                    general_items = await self._parse_trending_page(client, response.text, "general")
                    # è¿‡æ»¤AIç›¸å…³é¡¹ç›®
                    ai_keywords = ["ai", "llm", "gpt", "model", "ml", "deep", "neural", "agent"]
                    for item in general_items[:20]:
                        title_lower = item.title.lower()
                        if any(kw in title_lower for kw in ai_keywords):
                            items.append(item)
                            
        except Exception as e:
            log.error(f"GitHub Trendingæ”¶é›†å¤±è´¥: {e}")
        
        return items
    
    async def _fetch_readme_snippet(self, client: httpx.AsyncClient, repo_path: str) -> str:
        """
        è·å–ä»“åº“ README çš„å‰ 800 å­—ç¬¦ä½œä¸ºé¡¹ç›®æ¦‚è¦
        
        Args:
            client: HTTPå®¢æˆ·ç«¯
            repo_path: ä»“åº“è·¯å¾„ (owner/repo)
            
        Returns:
            README æ‘˜è¦æ–‡æœ¬
        """
        try:
            url = f"{self.API_BASE}/repos/{repo_path}/readme"
            response = await client.get(url, headers={
                "Accept": "application/vnd.github.v3.raw",
                "User-Agent": "DailyNews/1.0"
            })
            if response.status_code == 200:
                readme_text = response.text
                # å»æ‰ markdown å›¾ç‰‡/badge è¡Œï¼Œåªä¿ç•™æ–‡å­—
                import re
                lines = readme_text.split("\n")
                clean_lines = []
                for line in lines:
                    stripped = line.strip()
                    # è·³è¿‡çº¯å›¾ç‰‡è¡Œã€badgeè¡Œã€ç©ºHTMLæ ‡ç­¾è¡Œ
                    if stripped.startswith("![") or stripped.startswith("<img") or stripped.startswith("[!["):
                        continue
                    if stripped.startswith("<p align") or stripped.startswith("</p>"):
                        continue
                    clean_lines.append(line)
                clean_text = "\n".join(clean_lines).strip()
                return clean_text[:800]
        except Exception as e:
            log.debug(f"è·å– {repo_path} README å¤±è´¥: {e}")
        return ""
    
    async def _parse_trending_page(self, client: httpx.AsyncClient, html: str, topic: str) -> List[NewsItem]:
        """
        è§£æTrendingé¡µé¢ï¼Œå¹¶ä¸ºæ¯ä¸ªä»“åº“è·å– README æ‘˜è¦
        
        Args:
            client: HTTPå®¢æˆ·ç«¯
            html: é¡µé¢HTML
            topic: ä¸»é¢˜
            
        Returns:
            æ–°é—»åˆ—è¡¨
        """
        items = []
        soup = BeautifulSoup(html, "lxml")
        
        # æŸ¥æ‰¾ä»“åº“åˆ—è¡¨
        repo_list = soup.select("article.Box-row")
        
        for repo in repo_list:
            try:
                # ä»“åº“åç§°å’Œé“¾æ¥
                h2 = repo.select_one("h2 a")
                if not h2:
                    continue
                
                repo_path = h2.get("href", "").strip("/")
                repo_name = repo_path.replace("/", " / ")
                repo_url = f"https://github.com/{repo_path}"
                
                # æè¿°
                desc_elem = repo.select_one("p")
                description = desc_elem.get_text(strip=True) if desc_elem else ""
                
                # è¯­è¨€
                lang_elem = repo.select_one("[itemprop='programmingLanguage']")
                language = lang_elem.get_text(strip=True) if lang_elem else "Unknown"
                
                # Staræ•°
                star_elem = repo.select_one("a[href$='/stargazers']")
                stars = star_elem.get_text(strip=True) if star_elem else "0"
                
                # ä»Šæ—¥æ–°å¢Star
                today_stars_elem = repo.select_one("span.d-inline-block.float-sm-right")
                today_stars = today_stars_elem.get_text(strip=True) if today_stars_elem else ""
                
                # è·å– README æ‘˜è¦
                readme_snippet = await self._fetch_readme_snippet(client, repo_path)
                await asyncio.sleep(0.5)  # API è¯·æ±‚é—´éš”
                
                # æ„å»ºä¸°å¯Œçš„ä¸­æ–‡å†…å®¹
                content = f"""ğŸ“Œ é¡¹ç›®ï¼š{repo_name}
ğŸ”— åœ°å€ï¼š{repo_url}
ğŸ“ ç®€ä»‹ï¼š{description}
ğŸ’» è¯­è¨€ï¼š{language} | â­ {stars} | ğŸ“ˆ ä»Šæ—¥ +{today_stars}
ğŸ·ï¸ ä¸»é¢˜ï¼š{topic}

ğŸ“„ é¡¹ç›®è¯¦æƒ…ï¼š
{readme_snippet if readme_snippet else 'æš‚æ— è¯¦ç»†è¯´æ˜'}
"""
                
                item = NewsItem(
                    title=f"ğŸ”¥ GitHubçƒ­é—¨: {repo_name}",
                    content=content.strip(),
                    url=repo_url,
                    source="GitHub Trending",
                    published_at=datetime.now(),
                    category=NewsCategory.OPENSOURCE,
                    tags=["github", "trending", topic, language.lower()],
                    extra={
                        "repo_path": repo_path,
                        "language": language,
                        "stars": stars,
                        "today_stars": today_stars,
                        "topic": topic,
                        "description": description,
                        "readme_snippet": readme_snippet
                    }
                )
                
                items.append(item)
                
            except Exception as e:
                log.debug(f"è§£æä»“åº“å¤±è´¥: {e}")
                continue
        
        return items
    
    async def _collect_repos(self) -> List[NewsItem]:
        """
        æ”¶é›†æŒ‡å®šä»“åº“çš„æœ€æ–°å‘å¸ƒ
        
        Returns:
            ä»“åº“åŠ¨æ€åˆ—è¡¨
        """
        items = []
        
        async with httpx.AsyncClient(timeout=30) as client:
            for repo in self.repos[:10]:  # é™åˆ¶ä»“åº“æ•°é‡
                try:
                    # è·å–æœ€æ–°release
                    url = f"{self.API_BASE}/repos/{repo}/releases/latest"
                    response = await client.get(url, headers={
                        "Accept": "application/vnd.github.v3+json",
                        "User-Agent": "DailyNews/1.0"
                    })
                    
                    if response.status_code == 200:
                        release = response.json()
                        
                        # æ£€æŸ¥å‘å¸ƒæ—¶é—´
                        published_at = datetime.fromisoformat(
                            release["published_at"].replace("Z", "+00:00")
                        )
                        
                        # åªæ”¶é›†æœ€è¿‘24å°æ—¶çš„å‘å¸ƒ
                        from datetime import timedelta, timezone
                        if datetime.now(timezone.utc) - published_at > timedelta(hours=48):
                            continue
                        
                        content = f"""
**ç‰ˆæœ¬**: {release.get('tag_name', 'N/A')}
**åç§°**: {release.get('name', 'N/A')}
**å‘å¸ƒæ—¶é—´**: {published_at.strftime('%Y-%m-%d %H:%M')}

**æ›´æ–°å†…å®¹**:
{release.get('body', 'æ— æè¿°')[:500]}
"""
                        
                        item = NewsItem(
                            title=f"ğŸ“¦ {repo} å‘å¸ƒæ–°ç‰ˆæœ¬ {release.get('tag_name', '')}",
                            content=content.strip(),
                            url=release.get("html_url", f"https://github.com/{repo}"),
                            source=f"GitHub - {repo}",
                            published_at=published_at.replace(tzinfo=None),
                            category=NewsCategory.OPENSOURCE,
                            tags=["github", "release", repo.split("/")[0]],
                            extra={
                                "repo": repo,
                                "version": release.get("tag_name"),
                                "prerelease": release.get("prerelease", False)
                            }
                        )
                        
                        items.append(item)
                    
                    await asyncio.sleep(1)  # APIè¯·æ±‚é—´éš”
                    
                except Exception as e:
                    log.debug(f"è·å–ä»“åº“ {repo} releaseå¤±è´¥: {e}")
                    continue
        
        return items


# æµ‹è¯•å…¥å£
if __name__ == "__main__":
    async def test():
        config = {
            "topics": ["artificial-intelligence", "llm"],
            "repositories": ["langchain-ai/langchain", "openai/openai-python"],
            "since": "daily"
        }
        
        collector = GitHubCollector(config)
        items = await collector.collect()
        
        print(f"æ”¶é›†åˆ° {len(items)} æ¡æ–°é—»")
        for item in items[:5]:
            print(f"- {item.title}")
            print(f"  URL: {item.url}")
            print()
    
    asyncio.run(test())
